
# ØªØ·Ø¨ÙŠÙ‚ ÙˆÙŠØ¨ Streamlit Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø¹ Google Drive/Sheets
import streamlit as st
import re
import pandas as pd
from datetime import datetime, date
import pytz
import os
import json
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import gspread
from gspread.utils import rowcol_to_a1

# ğŸ“ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©

FOLDER_ID = '1bNtXXxT6D8jivdVT-ynL0xMfUyYfonaB'  # ID Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¬Ø°Ø±ÙŠ
SHEET_URL = 'https://docs.google.com/spreadsheets/d/1TvO_nb8UfHHb2NihLWZ1eVcQChnzFiWqBZeD2gc_D14/edit'
WORKSHEET_NAME = 'data4'

EMPLOYEE_LIST = ["1085","1403","1093","1088","1084","1087","1092","1402","1400","1094","1603","1604","1607","1086","1608","1610","1611","1612","1154","1151","1602"]
MIN_SIZE = 400 * 1024  # 400 KB
MAX_SIZE = 4000 * 1024 # 4 MB

# ğŸ” ØªÙ‡ÙŠØ¦Ø© Ø®Ø¯Ù…Ø§Øª Google Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… key.json
def initialize_services():
    # ---
    # Ù„Ø±ÙØ¹ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù„Ù‰ Streamlit Cloud:
    # Ø£Ø¶Ù Ù…ØªØºÙŠØ± Ø³Ø±Ù‘ÙŠ Ø¨Ø§Ø³Ù… GOOGLE_CREDENTIALS ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Secrets
    # ÙˆØ¶Ø¹ ÙÙŠÙ‡ ÙƒØ§Ù…Ù„ Ù…Ø­ØªÙˆÙ‰ Ù…Ù„Ù service account (key.json) ÙƒÙ†Øµ JSON
    # Ù…Ø«Ø§Ù„: ÙÙŠ ØµÙØ­Ø© secrets Ø£Ø¶Ù
    # GOOGLE_CREDENTIALS = '{...json...}'
    # ---
    creds_json = os.environ.get('GOOGLE_CREDENTIALS')
    if not creds_json:
        st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© GOOGLE_CREDENTIALS. ÙŠØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØªÙ‡ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Secrets.")
        st.stop()
    creds_dict = json.loads(creds_json)
    creds = Credentials.from_service_account_info(
        creds_dict,
        scopes=['https://www.googleapis.com/auth/drive',
                'https://www.googleapis.com/auth/spreadsheets']
    )
    sheet = gspread.authorize(creds)
    drive_service = build('drive', 'v3', credentials=creds)
    return drive_service, sheet

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_audio_files(drive_service, root_id, date_start, date_end, employee):
    rows = []
    query = f"'{root_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false"
    subfolders = drive_service.files().list(q=query, fields="files(id,name)").execute().get('files', [])
    for folder in subfolders:
        if not re.fullmatch(r"\d{8}", folder["name"]):
            continue
        folder_dt = datetime.strptime(folder["name"], "%Y%m%d").date()
        if not (date_start <= folder_dt <= date_end):
            continue
        query = f"'{folder['id']}' in parents and trashed=false and name contains '.wav'"
        files = drive_service.files().list(q=query, fields="files(id,name,size)").execute().get('files', [])
        for f in files:
            name = f["name"].lower()
            if any(tok in name for tok in ("(", ")", "copy")):
                continue
            size = int(f.get("size", 0))
            if not (MIN_SIZE <= size <= MAX_SIZE):
                continue
            base = re.sub(r"\.wav$", "", f["name"], flags=re.I)
            parts = base.split("-")
            if len(parts) < 5:
                continue
            # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¸Ù Ø§Ù„Ù…Ø­Ø¯Ø¯ ÙÙ‚Ø·
            if employee and (parts[2] != employee and parts[3] != employee):
                continue
            rows.append({
                "Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù": base,
                "Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù„Ù": f"https://drive.google.com/file/d/{f['id']}/view?usp=sharing",
            })
    df = pd.DataFrame(rows)
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_sheet_data(sheet, url, ws_name):
    sheet_id = re.search(r"/d/([a-zA-Z0-9-_]+)", url).group(1)
    ws = sheet.open_by_key(sheet_id).worksheet(ws_name)

    try:
        df_sheet = pd.DataFrame(ws.get_all_records())
    except:
        df_sheet = pd.DataFrame()

    # ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† "Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù")
    headers = ws.row_values(1)
    def _ensure(col, idx):
        if col not in headers:
            ws.update_cell(1, idx, col)
            headers.append(col)
            if not df_sheet.empty:
                df_sheet[col] = ""

    _ensure("Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù", 1)
    _ensure("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù„Ù", 2)
    _ensure("ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©", 3)

    if df_sheet.empty:
        df_sheet = pd.DataFrame(columns=headers)

    return df_sheet, ws

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_new_files(df_drive, df_sheet):
    new_df = df_drive if df_sheet.empty else df_drive[~df_drive["Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù"].isin(df_sheet["Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù"])]
    return new_df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def append_new_files(ws, df_sheet, new_df):
    if new_df.empty:
        return 0
    new_df = new_df.copy()
    new_df["ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©"] = datetime.now(pytz.timezone("Asia/Riyadh")).strftime("%Y-%m-%d")
    for col in df_sheet.columns:
        if col not in new_df.columns:
            new_df[col] = ""
    new_df = new_df[df_sheet.columns]
    ws.append_rows(values=new_df.values.tolist(), value_input_option="USER_ENTERED")
    return len(new_df)

class FileNameParser:
    @staticmethod
    def remove_parentheses(text):
        return re.sub(r'\s*\(\d+\)$', '', text)

    def parse(self, file_name):
        file_name = self.remove_parentheses(file_name)
        parts = file_name.split("-")

        if len(parts) < 5:
            print(f"âš ï¸ Ø§Ø³Ù… ØºÙŠØ± ØµØ§Ù„Ø­ Ù„Ù„ØªÙÙƒÙŠÙƒ: {file_name}")
            return None

        try:
            raw_id = parts[0]
            date = f"{raw_id[:4]}-{raw_id[4:6]}-{raw_id[6:8]}"
            time = f"{raw_id[8:10]}:{raw_id[10:12]}"

            number1 = parts[2].strip()
            number2 = parts[3].strip()

            if len(number1) <= 7:
                extension, mobile = number1, number2
            else:
                extension, mobile = number2, number1

            return {
                "ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©": date,
                "ÙˆÙ‚Øª Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©": time,
                "ØªØ±Ù…ÙŠØ²": raw_id,
                "Ø±Ù‚Ù… Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø©": extension,
                "Ø±Ù‚Ù… Ø§Ù„Ø¬ÙˆØ§Ù„": mobile,
                "Ù†ÙˆØ¹ Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©": parts[4].strip(),
            }
        except Exception as e:
            print(f"âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {file_name} ({e})")
            return None

class SheetManager:
    def __init__(self, sheet_service, sheet_url, worksheet_name):
        self.sheet_service = sheet_service
        self.sheet_url = sheet_url
        self.worksheet_name = worksheet_name
        self.df = pd.DataFrame()
        self.worksheet = None
        self.updated_indices = []
        self.orig_date_col = None

    def load_data(self):
        match = re.search(r'/d/([a-zA-Z0-9-_]+)', self.sheet_url)
        sheet_id = match.group(1)
        sheet = self.sheet_service.open_by_key(sheet_id)
        self.worksheet = sheet.worksheet(self.worksheet_name)

        try:
            self.df = pd.DataFrame(self.worksheet.get_all_records())
        except:
            self.df = pd.DataFrame(columns=["Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù"])

        if 'ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©' in self.df.columns:
            self.orig_date_col = self.df['ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©'].astype(str).fillna('')
        else:
            self.orig_date_col = pd.Series([''] * len(self.df))

    # Ù„Ø§ Ø·Ø¨Ø§Ø¹Ø© ÙÙŠ Streamlit

    def get_unarchived_rows(self):
        if 'ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©' in self.df.columns:
            return self.df[self.df['ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©'] == '']
        return self.df

    def update_metadata(self, parser):
        unarchived = self.get_unarchived_rows()
        self.updated_indices = []

        if unarchived.empty or 'Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù' not in self.df.columns:
            print("â„¹ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙÙˆÙ ØªØ­ØªØ§Ø¬ ØªØ­Ø¯ÙŠØ«.")
            return

        idxs = unarchived.index.tolist()
        names = self.df.loc[idxs, 'Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù'].astype(str).tolist()

        updated = 0
        for idx, name in zip(idxs, names):
            if not name.strip():
                continue
            parsed = parser.parse(name)
            if parsed:
                for key, value in parsed.items():
                    self.df.at[idx, key] = value
                self.updated_indices.append(idx)
                updated += 1

    # Ù„Ø§ Ø·Ø¨Ø§Ø¹Ø© ÙÙŠ Streamlit

    def write_back(self, batch_size=400):
        if self.df.empty:
        # Ù„Ø§ Ø·Ø¨Ø§Ø¹Ø© ÙÙŠ Streamlit
            return

        if not self.updated_indices:
        # Ù„Ø§ Ø·Ø¨Ø§Ø¹Ø© ÙÙŠ Streamlit
            return

        cols = list(self.df.columns)
        ncols = len(cols)

        if 'ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©' not in self.df.columns:
            # Ù„Ø§ Ø·Ø¨Ø§Ø¹Ø© ÙÙŠ Streamlit
            date_col_idx = None
            date_col_pos = None
        else:
            date_col_pos = self.df.columns.get_loc('ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙƒØ§Ù„Ù…Ø©')
            date_col_idx = date_col_pos + 1

        new_date_rows = []
        existing_date_rows = []
        for i in sorted(self.updated_indices):
            had_value = False
            if self.orig_date_col is not None and i < len(self.orig_date_col):
                had_value = str(self.orig_date_col.iloc[i]).strip() != ''
            (existing_date_rows if had_value else new_date_rows).append(i)

        def build_blocks(indices):
            blocks, start, prev = [], None, None
            for i in indices:
                if start is None:
                    start = prev = i
                elif i == prev + 1:
                    prev = i
                else:
                    blocks.append((start, prev))
                    start = prev = i
            if start is not None:
                blocks.append((start, prev))
            return blocks

        updated_rows = 0

        # A) ØµÙÙˆÙ ØªØ§Ø±ÙŠØ®Ù‡Ø§ ÙƒØ§Ù† ÙØ§Ø±ØºÙ‹Ø§
        for s, e in build_blocks(new_date_rows):
            r = s
            while r <= e:
                end = min(e, r + batch_size - 1)
                values = (
                    self.df.iloc[r:end+1][cols]
                    .fillna('')
                    .astype(object)
                    .values
                    .tolist()
                )
                self.worksheet.update(
                    range_name=f"A{r + 2}:{rowcol_to_a1(end + 2, ncols)}",
                    values=values,
                    value_input_option='USER_ENTERED'
                )
                updated_rows += (end - r + 1)
                r = end + 1

        # B) ØµÙÙˆÙ ØªØ§Ø±ÙŠØ®Ù‡Ø§ ÙƒØ§Ù† Ù…Ø¹Ø¨Ù‘Ø£
        if date_col_idx is not None and existing_date_rows:
            left_cols = cols[:date_col_pos]
            right_cols = cols[date_col_pos+1:]

            for s, e in build_blocks(existing_date_rows):
                r = s
                while r <= e:
                    end = min(e, r + batch_size - 1)

                    if left_cols:
                        left_values = (
                            self.df.iloc[r:end+1][left_cols]
                            .fillna('')
                            .astype(object)
                            .values
                            .tolist()
                        )
                        self.worksheet.update(
                            range_name=f"A{r + 2}:{rowcol_to_a1(end + 2, len(left_cols))}",
                            values=left_values,
                            value_input_option='USER_ENTERED'
                        )

                    if right_cols:
                        right_start_col = date_col_idx + 1
                        right_values = (
                            self.df.iloc[r:end+1][right_cols]
                            .fillna('')
                            .astype(object)
                            .values
                            .tolist()
                        )
                        self.worksheet.update(
                            range_name=f"{rowcol_to_a1(r + 2, right_start_col)}:{rowcol_to_a1(end + 2, ncols)}",
                            values=right_values,
                            value_input_option='USER_ENTERED'
                        )

                    updated_rows += (end - r + 1)
                    r = end + 1

    # Ù„Ø§ Ø·Ø¨Ø§Ø¹Ø© ÙÙŠ Streamlit

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ğŸš€ Streamlit Web App
def main():
    st.set_page_config(page_title="Google Drive Sync", layout="centered")
    st.title("Ù…Ø²Ø§Ù…Ù†Ø© Ù…Ù„ÙØ§Øª Google Drive Ù…Ø¹ Google Sheets")

    # ÙƒÙ„Ù…Ø© Ø§Ù„Ø³Ø±
    password = st.text_input("ÙƒÙ„Ù…Ø© Ø§Ù„Ø³Ø±", type="password")
    if password != "1234":
        st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ ÙƒÙ„Ù…Ø© Ø§Ù„Ø³Ø± Ø§Ù„ØµØ­ÙŠØ­Ø©.")
        st.stop()

    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙˆØ¸Ù
    employee = st.selectbox("Ø§Ø®ØªØ± Ø±Ù‚Ù… Ø§Ù„Ù…ÙˆØ¸Ù", options=[""] + EMPLOYEE_LIST, index=0)

    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ØªØ§Ø±ÙŠØ®
    col1, col2 = st.columns(2)
    with col1:
        date_from = st.date_input("ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©", value=date(2025, 8, 5), format="YYYY-MM-DD")
    with col2:
        date_to = st.date_input("ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ©", value=date(2025, 10, 20), format="YYYY-MM-DD")

    if date_from > date_to:
        st.error("ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø¨Ù„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ©.")
        st.stop()

    if st.button("Ù…Ø²Ø§Ù…Ù†Ø©", type="primary"):
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©..."):
            try:
                drive_service, sheet = initialize_services()
                date_start, date_end = sorted([date_from, date_to])
                df_drive = get_audio_files(drive_service, FOLDER_ID, date_start, date_end, employee if employee else None)
                df_sheet, ws = get_sheet_data(sheet, SHEET_URL, WORKSHEET_NAME)
                new_files = find_new_files(df_drive, df_sheet)
                added_count = append_new_files(ws, df_sheet, new_files)

                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ­Ø¯ÙŠØ«Ù‡Ø§
                sheet_mgr = SheetManager(sheet, SHEET_URL, WORKSHEET_NAME)
                sheet_mgr.load_data()
                parser = FileNameParser()
                sheet_mgr.update_metadata(parser)
                sheet_mgr.write_back(batch_size=400)

                st.success(f"ØªÙ…Øª Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø© Ø¨Ù†Ø¬Ø§Ø­!\nØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ù…Ø¶Ø§ÙØ©: {added_count}")
                if not df_drive.empty:
                    st.dataframe(df_drive)
                else:
                    st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø´Ø±ÙˆØ·.")
            except Exception as e:
                st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {e}")

if __name__ == "__main__":
    main()